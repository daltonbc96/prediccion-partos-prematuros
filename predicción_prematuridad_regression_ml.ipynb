{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daltonbc96/prediccion-partos-prematuros/blob/main/predicci%C3%B3n_prematuridad_regression_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3045ecef"
      },
      "source": [
        "# Analisis preliminar del potencial de utilizacion de los datos del SIP para el entrenamiento de modelos de *machine learning*"
      ],
      "id": "3045ecef"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89d075bd"
      },
      "source": [
        "El desarrollo de modelos de aprendizaje automático con datos del mundo real es una tarea desafiante con un gran potencial para contribuir a los retos a los que se enfrenta la sociedad. Concretamente, el área de la salud ha empezado a beneficiarse de los esfuerzos por desarrollar modelos predictivos dedicados a diversos fines, como el análisis de exploraciones por imagen, la estratificación del riesgo de la población o el descubrimiento de nuevos fármacos, entre otros. Si nos fijamos en el ámbito de la salud pública, es notoria la necesidad de realizar esfuerzos adicionales, de modo que podamos ampliar el uso de las tecnologías en este sentido. El potencial para ampliar el acceso a la atención sanitaria en la prevención de enfermedades y la promoción de la salud es enorme. Consciente de estos retos, la OPS, a través de su departamento de *Evidence and Intelligence for Action in Health* (EIH), conjuntamente con el *Latin American Center of Perinatology, Women and Reproductive Health* (CLAP/WR), están desarrollando un primer esfuerzo analítico para evaluar el potencial de utilizar los datos registrados en el Sistema de Información Perinatal (SIP) como fuente para entrenar algoritmos de aprendizaje automático sobre temas relacionados con la salud materno-infantil. La idea es permitir la formación de modelos que puedan utilizarse para mejorar la calidad de la atención prestada a mujeres y niños en la región de las Américas.\n",
        "<br>\n",
        "<br>\n",
        "Para poder entrenar modelos de aprendizaje automático sobre datos estructurados brutos, es necesario realizar varios pasos analíticos asociados a las etapas de extracción, limpieza, tratamiento y modelización. Por lo general, para desarrollar modelos de datos estructurados ha resultado fructífera la siguiente secuencia de pasos:\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "|         **1.** Carga de Información del SIP y Procesamiento de Datos<br>\n",
        "|         **2.** Análisis y Tratamiento de Datos Faltantes<br>\n",
        "|         **3.** Controles de Asociación<br>\n",
        "|         **4.** Evaluación de Modelos de *Machine Learning* para Problemas de Regresión<br>\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Teniendo en cuenta los puntos ejemplificados anteriormente, este informe pretende detallar la secuencia de pasos, así como los códigos en lenguaje python que se pueden utilizar para cubrir los elementos necesarios, con el fin de permitir la evaluación de la potencialidad de los datos existentes con el SIP para la estructuración de modelos de aprendizaje automático relacionados con la salud materno-infantil. Como primera área de interés, los esfuerzos aquí detallados se dirigen al desarrollo de modelos destinados a la estratificación del riesgo de la población en cuanto a la ocurrencia de nacimientos prematuros. Nuestro objetivo es, mediante los datos disponibles en el SIP, estimar la semana probable de parto con los datos disponibles al inicio de la gestación. Esta información puede ayudar a los profesionales sanitarios de la región en el tratamiento de las embarazadas con alto potencial de parto prematuro. En las secciones siguientes se detallan los elementos necesarios, en términos de rutinas de tratamiento, para cubrir cada una de las 4 etapas definidas anteriormente.\n",
        "<br>\n",
        "<br>"
      ],
      "id": "89d075bd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uecj8g1ETnWW"
      },
      "source": [
        "## 1 - Carga de Información del SIP y Procesamiento de Datos\n",
        "\n",
        "Esta primera etapa se enfoca en preparar el terreno para el análisis posterior, comenzando con la carga de las bibliotecas necesarias, los datos originales del SIP, y un diccionario de datos organizado en un archivo JSON. La importancia de esta fase radica en establecer una base sólida para el análisis de datos, asegurando que todas las herramientas y recursos necesarios estén disponibles y correctamente configurados. Este paso es crucial porque sentará las bases para todo el trabajo analítico posterior, garantizando la integridad y la accesibilidad de los datos."
      ],
      "id": "Uecj8g1ETnWW"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgz_D5BNS_fp"
      },
      "source": [
        "\n",
        "###  1.1 Carga de Bibliotecas, Datos y Diccionario\n",
        "En este bloque de código, se importan bibliotecas esenciales para el análisis de datos y el aprendizaje automático. Bibliotecas como Pandas y NumPy son fundamentales para la manipulación y análisis de datos. Sweetviz se utiliza para la visualización y análisis exploratorio, mientras que Scikit-NA, Datapane y Plotly Express ayudan en el tratamiento de datos faltantes y visualizaciones interactivas. Se emplea una librería específica, 'pregnant', que incluye funciones personalizadas para el procesamiento de datos.\n",
        "\n",
        "El siguiente paso es la carga de los datos originales del SIP utilizando Pandas  y un archivo JSON que actúa como diccionario de datos. Este diccionario es vital para entender la estructura y el significado de los datos."
      ],
      "id": "Cgz_D5BNS_fp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a626vYZT71D8"
      },
      "outputs": [],
      "source": [
        "!pip install sweetviz scikit_na unidecode datapane scikit-optimize shap"
      ],
      "id": "a626vYZT71D8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y54QYkEjOmYb"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.append(\"/content/drive/MyDrive/Prematuros/personalFunctions\")"
      ],
      "id": "y54QYkEjOmYb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc169221"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import sweetviz as sv\n",
        "sv.config_parser.read(\"sweetviz_config.ini\")\n",
        "import scikit_na as na\n",
        "import datapane as dp\n",
        "import plotly.express as px\n",
        "from pregnant import process_data, get_types, dummify_categorical_columns, rename_columns, missing_columns, plot_missing_data_and_unique_value, plot_corr_graph, identificar_columnas_no_numericas, graficar_matriz_correlacion, matriz_correlacion_optimizada\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import shap\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "#df = pd.read_csv('/content/drive/MyDrive/Prematuros/Datos Originales/pregnancie.csv', low_memory=False)\n",
        "dictionary = json.load(open(\"/content/drive/MyDrive/Prematuros/JSON Setup/dictionary_pregnancie.json\",\"rb\"))\n",
        "#df"
      ],
      "id": "bc169221"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "373bb197"
      },
      "source": [
        "###  1.2 Exclusión de Variables sin Definición en el Diccionario\n",
        "\n",
        "En esta etapa, se procede a eliminar del conjunto de datos las variables que no están definidas en el diccionario de datos proporcionado. Este paso es crucial para garantizar la consistencia y la relevancia de los análisis posteriores. Las variables indefinidas, como '0510', '0616', '0630', entre otras, son excluidas del conjunto de datos. Esta exclusión se basa en la premisa de que si una variable no está definida claramente, su contribución al análisis puede ser cuestionable o su interpretación ambigua, lo que podría afectar la calidad del modelo de aprendizaje automático."
      ],
      "id": "373bb197"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c9a4f71"
      },
      "outputs": [],
      "source": [
        "variables_indefinidas = ['0510','0616','0630','0634','0687','0747','0789','A059','A060','A061','A062','A069','A077','A078','A154',\n",
        "                         'A157','A159','A160','A161','A162','A163','A173','A184','A193','A194','A195','A196','A197','A199','A216',\n",
        "                         'A217','V022','V026','V027','V031','V032', 'V033','V034','V035','V036','V037','V038','V039','V040','V041',\n",
        "                         'V042','V043','V044','V045','V046','V047','V048','V049','V050', 'V051','V052','V053','V054','V055','V056',\n",
        "                         'V057','V058','V059','V060','V061','V062','V063','V064','V065','V151','V273','V274', 'V275','V276','V277',\n",
        "                         'V278','V279','V280','_gestaId','_motherId']\n",
        "df.drop(variables_indefinidas, inplace=True, axis=1)"
      ],
      "id": "0c9a4f71"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1015078a"
      },
      "source": [
        "###  1.3 Ajuste de Tipado de las Variables, Etiquetas y Renombramiento de Columnas\n",
        "\n",
        "Se desarrolló una función de procesamiento de datos que recibe los datos y el diccionario de datos. Con base en las propiedades descritas en el diccionario, los datos se convierten en el formato correcto, pudiendo ser categórico, numérico, booleano, fecha y hora o cadena de texto. Además, se utiliza la información del diccionario para renombrar las variables, facilitando así su interpretación. El estándar usado es el nombre original de la variable seguido de un guion bajo y el significado de la variable, lo que ayuda a comprender mejor cada elemento del conjunto de datos."
      ],
      "id": "1015078a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30f97196"
      },
      "outputs": [],
      "source": [
        "#processed_df, cols_in_data_not_in_json, cols_in_json_not_in_data = process_data(raw_data = df, json_dictionary = dictionary, check_columns_in_data_not_in_json = True, check_columns_in_json_not_in_data = True)\n",
        "#processed_df = rename_columns(processed_df, dictionary)\n",
        "#processed_df.to_parquet('/content/drive/MyDrive/Prematuros/processed_df.parquet.gzip')\n",
        "processed_df = pd.read_parquet('/content/drive/MyDrive/Prematuros/processed_df.parquet.gzip')\n",
        "processed_df"
      ],
      "id": "30f97196"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1876620b"
      },
      "source": [
        "###  1.4 Análisis Descriptivo de los Datos Procesados\n",
        "\n",
        "Se elaboró un informe de *Exploratory Data Analysis* (EDA) del conjunto de datos procesados antes de proceder a procesamientos más profundos que podrían alterar la estructura original de los datos. Este informe, disponible en formato HTML, se puede consultar en un archivo adjunto al cuaderno Jupyter. En él, es posible observar las características generales de cada variable, análisis de datos faltantes y visualizaciones gráficas. Este paso es fundamental para entender la naturaleza de los datos con los que se trabajará en las fases posteriores del modelado."
      ],
      "id": "1876620b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "262e36c5"
      },
      "outputs": [],
      "source": [
        "#report = sv.analyze(processed_df, pairwise_analysis=\"off\")\n",
        "#report.show_html('/content/drive/MyDrive/Prematuros/processed_df_report.html', open_browser=False)\n",
        "#report.show_notebook()\n",
        "HTML(filename=\"/content/drive/MyDrive/Prematuros/processed_df_report.html\")"
      ],
      "id": "262e36c5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1203de7"
      },
      "source": [
        "## 2 - Análisis y Tratamiento de Datos Faltantes\n",
        "En esta sección, nos enfocamos en el tratamiento y preparación de los datos para su posterior utilización en modelos de *machine learning*. Este proceso es fundamental para asegurar la calidad y la eficiencia de los modelos predictivos. Se abordan diversos aspectos como el análisis y tratamiento de datos faltantes, la remoción de información irrelevante y la transformación de variables categóricas, culminando con la creación de un reporte analítico detallado de los datos."
      ],
      "id": "b1203de7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4dad2ff"
      },
      "source": [
        "### 2.1 Análisis de Datos Faltantes\n",
        "Es importante analizar y tratar los datos faltantes, ya que pueden influir significativamente en el rendimiento del modelo. Este análisis se divide en varias subsecciones para abordar diferentes aspectos de los datos faltantes."
      ],
      "id": "f4dad2ff"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBZ0Z0BaWo_v"
      },
      "source": [
        "#### 2.1.1 Análisis General de Datos Faltantes\n",
        "Se realiza una revisión general de los datos faltantes en todo el conjunto de datos transformados. Este análisis proporciona una visión general del nivel de completitud de los datos y ayuda a identificar posibles problemas o áreas que requieren atención especial."
      ],
      "id": "QBZ0Z0BaWo_v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38bd7a4c"
      },
      "outputs": [],
      "source": [
        "missing_all_data = pd.DataFrame(na.summary(processed_df, per_column=False))\n",
        "missing_all_data"
      ],
      "id": "38bd7a4c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjYEJo1QWm36"
      },
      "source": [
        "####  2.1.2 Análisis Detallado por Columna\n"
      ],
      "id": "HjYEJo1QWm36"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFW64-ybYCNc"
      },
      "source": [
        "##### 2.1.2.1 Gráfico\n",
        "\n",
        "Se presenta un gráfico de barras que muestra la distribución del porcentaje de datos faltantes en cada columna. Esta visualización es útil para identificar rápidamente las columnas con niveles altos de datos faltantes, lo que puede indicar la necesidad de un tratamiento adicional o la exclusión de ciertas variables."
      ],
      "id": "NFW64-ybYCNc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bea63d2"
      },
      "outputs": [],
      "source": [
        "fig_missing, fig_unique = plot_missing_data_and_unique_value(processed_df, sort_descending=True, missing_threshold=70, figure_height=780)\n",
        "fig_missing.show()\n",
        "#fig_unique.show()"
      ],
      "id": "6bea63d2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZDh-QvLYMG2"
      },
      "source": [
        "##### 2.1.2.2 Tabla\n",
        "Esta tabla proporciona una visión más detallada de los valores faltantes en cada columna, complementando la información visual del gráfico y ofreciendo una perspectiva más profunda de los datos."
      ],
      "id": "OZDh-QvLYMG2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0f7203a"
      },
      "outputs": [],
      "source": [
        "missing_per_column = na.summary(processed_df)\n",
        "dp.DataTable(missing_per_column.transpose())"
      ],
      "id": "b0f7203a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDYaVLZM2tou"
      },
      "source": [
        "####  2.1.3 Análisis de Datos Faltantes de la Variable Objetivo\n",
        "Se analiza la variable objetivo en términos de valores faltantes y su distribución. Conocer la cantidad de datos faltantes y la naturaleza de la distribución de la variable objetivo es crucial para el desarrollo de modelos predictivos efectivos."
      ],
      "id": "tDYaVLZM2tou"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Y7bsGyFYkqj"
      },
      "outputs": [],
      "source": [
        "processed_df[\"0198_edad_gestacional_al_parto\"].hist()\n",
        "missing_count = processed_df[\"0198_edad_gestacional_al_parto\"].isnull().sum()\n",
        "print(f\"Número de valores missing: {missing_count}\")\n",
        "processed_df[\"0198_edad_gestacional_al_parto\"].describe()"
      ],
      "id": "-Y7bsGyFYkqj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmAHhHrodOgy"
      },
      "source": [
        "### 2.2 Remoción de Información y Tratamiento de Datos Faltantes\n",
        "\n",
        "En esta sección, nos enfocaremos en la remoción de información innecesaria y en el tratamiento detallado de los datos faltantes. Este proceso es crucial para optimizar la calidad de los datos y garantizar la precisión en los modelos de *machine learning*."
      ],
      "id": "YmAHhHrodOgy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtvWT7Ly2cOf"
      },
      "source": [
        "####  2.2.1 Remoción de Informaciones Irrelevantes\n",
        "Se identifican y eliminan variables y registros que no aportan valor o que pueden sesgar los resultados del modelo.\n",
        "\n",
        "* **Pasos para la Remoción de Informaciones**:\n",
        "Identificación de Columnas No Numéricas: Eliminamos aquellas columnas que no contienen datos numéricos, ya que nuestro modelo se centrará en variables cuantitativas.\n",
        "* **Eliminación de Columnas con Alto Porcentaje de Valores Faltantes**: Las columnas con más del 70% de datos faltantes son eliminadas, considerando que su baja completitud puede afectar negativamente la calidad del modelo.\n",
        "* **Limpieza de Filas Específicas**: Se remueven filas donde la 'edad gestacional al parto' es nula o anormalmente alta (>44 semanas), ya que estos datos podrían ser incorrectos o irrelevantes para el análisis. También eliminó los casos que faltaban de esta variable."
      ],
      "id": "BtvWT7Ly2cOf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c53f3d1f"
      },
      "outputs": [],
      "source": [
        "#Columns\n",
        "# Identificar las columnas no numéricas\n",
        "non_numerical_columns = identificar_columnas_no_numericas(processed_df)\n",
        "\n",
        "# Encontrar columnas con alto porcentaje de valores faltantes\n",
        "cols_high_missing, cols_single_value, combined_cols = missing_columns(processed_df, missing_threshold=70)\n",
        "\n",
        "# Combinar las listas de columnas a eliminar\n",
        "columns_drop = list(set(cols_high_missing + non_numerical_columns))\n",
        "\n",
        "# Eliminar las columnas especificadas de 'processed_df' para crear 'df_clean'\n",
        "df_clean = processed_df.drop(columns=columns_drop)\n",
        "\n",
        "# Rows\n",
        "# Limpieza de filas en 'df_clean'\n",
        "# Eliminar filas donde '0198_edad_gestacional_al_parto' es NaN\n",
        "df_clean = df_clean[df_clean['0198_edad_gestacional_al_parto'].notna()]\n",
        "\n",
        "# Eliminar filas donde '0198_edad_gestacional_al_parto' es mayor a 44 (semanas)\n",
        "df_clean = df_clean[df_clean[\"0198_edad_gestacional_al_parto\"] <= 44]\n",
        "df_clean"
      ],
      "id": "c53f3d1f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8xH181Nul5u"
      },
      "source": [
        "####  2.2.2 Tratamiento de Datos Faltantes\n",
        "\n",
        "El tratamiento de los datos faltantes se realiza con el fin de completar el conjunto de datos y prepararlo para el análisis predictivo. Se utilizan técnicas avanzadas para asegurar que el tratamiento de estos valores faltantes sea lo más preciso posible, sin introducir sesgos innecesarios. A continuación, se detalla cada paso del proceso."
      ],
      "id": "K8xH181Nul5u"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxhWDCf-vYrZ"
      },
      "source": [
        "##### Paso 1: Preparación para la Imputación\n",
        "\n",
        "* **Exclusión de las Columnas Objetivo**: Se retiran temporalmente las columnas objetivo del conjunto de datos para evitar su alteración durante el proceso de imputación.\n",
        "* **Marcado de Datos Faltantes en Columnas Numéricas**: Para cada columna numérica, se añade una nueva columna indicadora que marca si el valor original era faltante con '_was_missing'.\n",
        "* **Selección de Columnas Categóricas**: Identificación de todas las columnas categóricas, excluyendo aquellas marcadas como '_was_missing'.\n",
        "* **Transformación a Variables Dummy**: Conversión de variables categóricas en un formato adecuado para modelos de machine learning mediante la creación de variables binarias ('dummy variables') para cada categoría.\n"
      ],
      "id": "kxhWDCf-vYrZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eNvGVS4SsXN"
      },
      "outputs": [],
      "source": [
        "# Excluir las columnas objetivo de las transformaciones\n",
        "columnas_objetivo = ['0198_edad_gestacional_al_parto']\n",
        "df_clean_excluido = df_clean.drop(columns=columnas_objetivo)\n",
        "\n",
        "# Paso 1: Identificar las columnas numéricas y crear las columnas indicadoras\n",
        "columnas_numericas = df_clean_excluido.select_dtypes(include=[np.number]).columns\n",
        "for col in columnas_numericas:\n",
        "    df_clean_excluido[col + '_was_missing'] = df_clean_excluido[col].isnull()\n",
        "\n",
        "# Seleccionar solo las columnas categóricas originales, excluyendo las que tienen el sufixo '_was_missing'\n",
        "columnas_categoricas = df_clean_excluido.select_dtypes(include=['object', 'category', 'boolean']).columns\n",
        "columnas_categoricas = [col for col in columnas_categoricas if not col.endswith('_was_missing')]\n",
        "df_clean_dummy = pd.get_dummies(df_clean_excluido, columns=columnas_categoricas, dummy_na=True, drop_first=True)\n",
        "df_clean_dummy"
      ],
      "id": "8eNvGVS4SsXN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvqJTr1rvvKr"
      },
      "source": [
        "##### Paso 2: Imputación de Datos Numéricos\n",
        "Aplicación de MICE: Uso de la técnica de Imputación Múltiple por Cadena de Ecuaciones (MICE) para llenar los valores faltantes en las columnas numéricas.  Esta técnica es un enfoque avanzado para la imputación de datos faltantes que modela cada variable con valores faltantes como una función de otras variables y utiliza esa estimación para imputar los valores faltantes. La imputación se realiza de manera iterativa, lo que permite un tratamiento más sofisticado y preciso de los datos faltantes.\n",
        "\n",
        "En este caso, se utiliza un **DecisionTreeRegressor** como estimador en el proceso de imputación, debido a su eficacia en el manejo de relaciones no lineales entre variables. El argumento **max_iter** especifica el número máximo de iteraciones de imputación, lo que permite un equilibrio entre precisión y eficiencia computacional. Tras la imputación, se verifica la presencia de datos faltantes para asegurar que el conjunto de datos esté completo."
      ],
      "id": "mvqJTr1rvvKr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWG_Mp8pXUpx"
      },
      "outputs": [],
      "source": [
        "imputer = IterativeImputer(estimator=DecisionTreeRegressor(), max_iter=10, random_state=32, initial_strategy=\"median\")\n",
        "imputed_values = imputer.fit_transform(df_clean_dummy[columnas_numericas])\n",
        "\n",
        "# Crear un DataFrame con los valores imputados para las columnas numéricas\n",
        "df_imputed_numeric = pd.DataFrame(imputed_values, columns=columnas_numericas, index=df_clean.index)"
      ],
      "id": "iWG_Mp8pXUpx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_PHpug81jN0"
      },
      "source": [
        "##### Paso 3: Combinación de Datos Tratados\n",
        "\n",
        "La siguiente información se combinó en un único dataframe:\n",
        "\n",
        "* **Reintegración de Columnas Objetivo**: Se reincorporan las columnas objetivo al conjunto de datos.\n",
        "* **Combinación con Datos Imputados y Dummificados**: Los datos numéricos imputados se combinan con las variables dummificadas y las columnas indicadoras.\n",
        "* **Conversión de Tipos de Datos**: Todas las columnas se convierten al tipo float64 para mantener la consistencia en el conjunto de datos.\n",
        "* **Eliminación de Columnas con Variación Nula**: Se identifican y eliminan las columnas que contienen un único valor, ya que no aportan información relevante para el modelo."
      ],
      "id": "j_PHpug81jN0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8IiMOXsS18f"
      },
      "outputs": [],
      "source": [
        "#Combinar los resultados con las columnas dummizadas, las columnas indicadoras y las columnas objetivo\n",
        "df_imputed = pd.concat([df_clean[columnas_objetivo], df_clean_dummy.drop(columnas_numericas, axis=1), df_imputed_numeric], axis=1)\n",
        "# Convertendo todas as colunas para float64\n",
        "df_imputed = df_imputed.astype('float64')\n",
        "\n",
        "# Identificar colunas com apenas um valor único\n",
        "colunas_sem_variacao = [col for col in df_imputed.columns if df_imputed[col].nunique() == 1]\n",
        "\n",
        "# Remover essas colunas do DataFrame\n",
        "df_imputed = df_imputed.drop(columns=colunas_sem_variacao)\n",
        "\n",
        "df_imputed"
      ],
      "id": "y8IiMOXsS18f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jCycIxeuYNR"
      },
      "outputs": [],
      "source": [
        "#df_imputed.to_parquet('/content/drive/MyDrive/Prematuros/df_imputed.parquet.gzip')\n",
        "df_imputed = pd.read_parquet('/content/drive/MyDrive/Prematuros/df_imputed.parquet.gzip')\n",
        "df_imputed"
      ],
      "id": "2jCycIxeuYNR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6Bb6cI12bSf"
      },
      "source": [
        "##### Paso 4: Verificación Final de Datos Faltantes\n",
        " Se realiza una última revisión para asegurar que no queden valores faltantes en el conjunto de datos. Esta verificación es crucial para confirmar que el proceso de tratamiento ha sido exitoso."
      ],
      "id": "d6Bb6cI12bSf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6JHUPCISsWC"
      },
      "outputs": [],
      "source": [
        "missing_per_column = na.summary(df_imputed)\n",
        "dp.DataTable(missing_per_column.transpose())"
      ],
      "id": "D6JHUPCISsWC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UEd5Hij3LPU"
      },
      "source": [
        "##### Paso 5: Reporte Analítico de los Datos Tratados\n",
        "\n",
        "Se genera un reporte detallado de EDA para el conjunto de datos tratado. Este reporte proporciona una visión integral de los datos, incluyendo la distribución de las variables, la correlación entre ellas y otras estadísticas importantes. Este informe, disponible en formato HTML, se puede consultar en un archivo adjunto al cuaderno Jupyter."
      ],
      "id": "5UEd5Hij3LPU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ximZixnbvalR"
      },
      "outputs": [],
      "source": [
        "#report = sv.analyze(df_imputed, pairwise_analysis=\"off\")\n",
        "#report.show_html('/content/drive/MyDrive/Prematuros/df_imputed_report.html', open_browser=False)\n",
        "#report.show_notebook()\n",
        "HTML(filename=\"/content/drive/MyDrive/Prematuros/df_imputed_report.html\")"
      ],
      "id": "ximZixnbvalR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6dMlduOIcwS"
      },
      "source": [
        "## 3 - Controles de Asociación\n",
        "\n",
        "Esta etapa es crucial para la preparación de los datos antes del modelado en machine learning. Aquí, se enfoca en evaluar y controlar las relaciones entre las variables, identificando correlaciones significativas con la variable objetivo y entre las variables predictoras. Este análisis es esencial para garantizar que el modelo final sea preciso y no esté sesgado por relaciones redundantes o irrelevantes."
      ],
      "id": "Z6dMlduOIcwS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5hBPzxnImKZ"
      },
      "source": [
        "### 3.1 - Cálculo de correlaciones basadas en la variable objetivo\n",
        "\n",
        "Para investigar qué variables están asociadas con la variable objetivo, se realizó un análisis de correlación de Pearson. En este análisis, mantuvimos en la matriz de correlaciones sólo las variables que tenían una relación mínima de 0,1 o más con la variable objetivo. Este paso ayuda a identificar qué variables tienen una relación significativa con la variable objetivo, proporcionando una base sólida para seleccionar características relevantes para el modelo de aprendizaje automático. A continuación, estas correlaciones se visualizan en un mapa de calor, lo que facilita la interpretación y la selección de las variables predictoras más relevantes para el modelo."
      ],
      "id": "A5hBPzxnImKZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geGlfZM05U2J"
      },
      "outputs": [],
      "source": [
        "matriz_corr_abs = matriz_correlacion_optimizada(df_imputed, '0198_edad_gestacional_al_parto', 0.1, ignorar_columnas = [])\n",
        "features_with_relation = matriz_corr_abs.dropna(how='all', axis=0).dropna(how='all', axis=1).columns\n",
        "\n",
        "graficar_matriz_correlacion(matriz_corr_abs)"
      ],
      "id": "geGlfZM05U2J"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-0X-P3NJpjN"
      },
      "source": [
        "### 3.2 Análisis de Combinación Lineal\n",
        "\n",
        "Se establece un umbral de correlación de 0.95 ou mas para identificar relaciones lineales perfectas entre las variables predictoras. El propósito de este análisis es identificar pares de variables que están tan fuertemente correlacionadas que efectivamente proporcionan la misma información al modelo. Se utiliza un gráfico tipo heatmap para visualizar estas correlaciones y facilitar la identificación de variables redundantes."
      ],
      "id": "0-0X-P3NJpjN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZnLTjUGQ_R7"
      },
      "outputs": [],
      "source": [
        "filtered_matrix = matriz_corr_abs[((abs(matriz_corr_abs) >= 0.95))]\n",
        "# Substituir valores na diagonal (autocorrelação) por NaN\n",
        "np.fill_diagonal(filtered_matrix.values, np.nan)\n",
        "# Contar o número de variáveis não totalmente missing (ignorando autocorrelações)\n",
        "combo_linear_columns = filtered_matrix.dropna(how='all', axis=0).dropna(how='all', axis=1).columns\n",
        "num_variaveis = len(combo_linear_columns)\n",
        "print(f\"Número de variables con problemas de Combo linear: {num_variaveis}\")\n",
        "graficar_matriz_correlacion(filtered_matrix)"
      ],
      "id": "qZnLTjUGQ_R7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf6HrRRwdVYG"
      },
      "source": [
        "### 3.3 Verificación de Multicolinearidad con VIF\n",
        "La multicolinearidad es un fenómeno en el que las variables predictoras en un modelo estadístico están altamente correlacionadas. Esto puede llevar a problemas en la interpretación de los resultados y afectar la precisión del modelo. Para evaluar la multicolinearidad, se utiliza el Factor de Inflación de la Varianza (VIF, por sus siglas en inglés). El VIF mide cuánto se infla la varianza de un coeficiente estimado si tus variables predictoras están correlacionadas. Si el VIF es alto, significa que hay una fuerte correlación entre esa variable predictora y las demás.\n",
        "\n",
        "En este proceso, se calcula el VIF para cada variable predictora, excluyendo la variable objetivo para no sesgar los resultados. La práctica estándar es revisar las variables con los VIF más altos, eliminar la más problemática y recalcular el VIF para el resto de las variables. Este proceso se repite hasta que todas las variables restantes tengan valores de VIF aceptables, generalmente un VIF menor a 5 o 10, dependiendo de los estándares utilizados."
      ],
      "id": "Wf6HrRRwdVYG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXmzN9CdUBNj"
      },
      "outputs": [],
      "source": [
        "df_independent = df_imputed[features_with_relation].drop(['0198_edad_gestacional_al_parto'], axis=1)\n",
        "vif2_data = pd.DataFrame()\n",
        "vif2_data[\"Feature\"] = df_independent.columns\n",
        "\n",
        "# calculating VIF for each feature\n",
        "vif2_data[\"VIF\"] = [variance_inflation_factor(df_independent.values, i)\n",
        "                          for i in range(len(df_independent.columns))]\n",
        "vif2_data"
      ],
      "id": "BXmzN9CdUBNj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jRC_MlEJvb2"
      },
      "source": [
        "### 3.4 Selección de Variables para Incorporar a los Modelos\n",
        "Por último, se seleccionan las variables identificadas en la etapa anterior que muestran una asociación y se excluyen del conjunto de datos las que están perfectamente o casi perfectamente correlacionadas (combo lineal) y las que muestran multicolinealidad. Este paso es esencial para garantizar que el modelo no incluya información redundante, lo que podría afectar a su rendimiento."
      ],
      "id": "6jRC_MlEJvb2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc6F_-XJ06qL"
      },
      "outputs": [],
      "source": [
        "features_with_combolinear = []\n",
        "features_with_multicolinealidad = []\n",
        "\n",
        "features_with_relation_atualizada = [elemento for elemento in features_with_relation if elemento not in features_with_combolinear and elemento not in features_with_multicolinealidad]"
      ],
      "id": "Qc6F_-XJ06qL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZWxbbzhkPb3"
      },
      "source": [
        "## 4 - Evaluación de Modelos de Machine Learning para Problemas de Regresión\n",
        "En esta sección, se evalúan diferentes modelos de *machine learning* específicamente diseñados para resolver problemas de regresión. El objetivo principal es predecir la semana de parto, un problema de regresión por su naturaleza continua. La importancia de esta sección radica en identificar el modelo que mejor prediga el momento del parto, lo cual es crucial para planificar intervenciones médicas adecuadas y mejorar los resultados de salud materno-infantil."
      ],
      "id": "1ZWxbbzhkPb3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv-Gx7Ifk3em"
      },
      "source": [
        "### 4.1 Separación de Características y Etiquetas\n",
        "En esta parte, se procede a separar las variables del conjunto de datos en dos grupos: las variables predictoras (X) y la variable objetivo (Y), que en este caso es la semana de parto. Esta separación es un paso fundamental en la preparación de datos para el entrenamiento de modelos de machine learning."
      ],
      "id": "xv-Gx7Ifk3em"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDKw-_lEkuWO"
      },
      "outputs": [],
      "source": [
        "x = df_imputed[features_with_relation_atualizada].drop(['edad_gestacional_al_parto_binary', '0198_edad_gestacional_al_parto'], axis=1)\n",
        "y = df_imputed[\"0198_edad_gestacional_al_parto\"]"
      ],
      "id": "vDKw-_lEkuWO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh0CgHhGDmEf"
      },
      "source": [
        "### 4.2 Selección de Algoritmos a Probar\n",
        "Se definirán los algoritmos a probar, seleccionando un representante de cada una de las siguientes familias de algoritmos:\n",
        "* **Algoritmos de Regresión**: Ideales para predecir variables continuas, como la semana de parto.\n",
        "* **Algoritmos Basados en Instancias**: Útiles en casos donde las relaciones entre datos son más importantes que la estructura subyacente.\n",
        "* **Algoritmos de Regularización**: Ayudan a prevenir el sobreajuste en modelos complejos.\n",
        "* **Algoritmos Bayesianos**: Proporcionan estimaciones probabilísticas, útiles para entender la incertidumbre en las predicciones.\n",
        "* **Redes Neuronales Artificiales**: Ofrecen flexibilidad y potencia para capturar relaciones complejas en los datos.\n",
        "* **Algoritmos de Ensemble**: Combinan las fortalezas de varios modelos para mejorar el rendimiento general.\n",
        "\n",
        "\n"
      ],
      "id": "vh0CgHhGDmEf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dk37-ncqmlwo"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "\n",
        "MLA = [\n",
        "#Regression Algorithms\n",
        "#LinearRegression(),\n",
        "\n",
        "#Instance-based Algorithms\n",
        "#SVR(),\n",
        "\n",
        "#Regularization Algorithms\n",
        "#ElasticNet(),\n",
        "\n",
        "#Decision Tree Algorithms\n",
        "#DecisionTreeRegressor(),\n",
        "\n",
        "#Bayesian Algorithms\n",
        "#BayesianRidge(),\n",
        "\n",
        "#Artificial Neural Network Algorithms\n",
        "#MLPRegressor(),\n",
        "\n",
        "# Ensemble Algorithms\n",
        "GradientBoostingRegressor()\n",
        "\n",
        "]"
      ],
      "id": "Dk37-ncqmlwo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oKUz4UXD1x_"
      },
      "source": [
        "### 4.3 Prueba de Diferentes Algoritmos de Machine Learning\n",
        "\n",
        "En esta sección, abordamos la evaluación y comparación de diferentes algoritmos de *machine learning* para el problema de regresión, específicamente la predicción de la semana de parto. A continuación, se detalla cada paso del proceso, siguiendo el código proporcionado y centrando la atención en la optimización bayesiana de hiperparámetros con BayesSearchCV.\n",
        "\n",
        "**Definición del Espacio de Hiperparámetros**\n",
        "\n",
        "Cada algoritmo de *machine learning* tiene hiperparámetros específicos que influyen en su rendimiento. En esta fase, se define un \"espacio de hiperparámetros\" para cada algoritmo, que determina los posibles valores o rangos de valores que estos hiperparámetros pueden tomar. Por ejemplo:\n",
        "\n",
        "- Para una Regresión Lineal (LinearRegression), no se requiere ajustar hiperparámetros esenciales.\n",
        "- Para un SVR (Support Vector Regressor), se definen rangos para parámetros como 'C', 'epsilon' y el tipo de 'kernel'.\n",
        "- Para otros algoritmos como ElasticNet, DecisionTreeRegressor, BayesianRidge, MLPRegressor y GradientBoostingRegressor, se establecen rangos específicos para sus respectivos hiperparámetros.\n",
        "\n",
        "Estos espacios de hiperparámetros serán utilizados posteriormente en la optimización bayesiana mediante BayesSearchCV, buscando los valores óptimos que maximicen el rendimiento del modelo.\n",
        "<br>\n",
        "<br>\n",
        "**Validación Interna de los Modelos Predictivos**\n",
        "\n",
        "La validación interna de los modelos se realiza mediante un enfoque de validación cruzada basado en el nivel de participante, con dos divisiones (folds) y cinco repeticiones para la separación de los datos. Este método garantiza que cada muestra del conjunto de datos se utilice tanto en el entrenamiento como en la validación de los modelos, proporcionando una evaluación robusta y fiable del rendimiento del modelo.\n",
        "<br>\n",
        "<br>\n",
        "**Proceso de Prueba y Optimización**\n",
        "\n",
        "El proceso de prueba y optimización se lleva a cabo de la siguiente manera:\n",
        "1. **Bucle de Validación Cruzada**: Se inicia un bucle para realizar la validación cruzada y la optimización bayesiana en cada repetición. Se utiliza KFold de scikit-learn para dividir los datos en conjuntos de entrenamiento y prueba.\n",
        "2. **Optimización Bayesiana con BayesSearchCV**: Para cada algoritmo, se aplica BayesSearchCV, una técnica de optimización que utiliza un enfoque bayesiano para encontrar los mejores hiperparámetros. Este método es más eficiente que las técnicas tradicionales como la búsqueda en cuadrícula, ya que se adapta y aprende de cada iteración para mejorar la búsqueda. Esta técnica ajusta iterativamente los hiperparámetros basándose en resultados anteriores, lo que permite una búsqueda más eficiente y efectiva de los mejores parámetros para cada modelo.\n",
        "\n",
        "\n",
        "3. **Evaluación y Comparación de Modelos**: Una vez entrenados y optimizados los modelos, se realizan predicciones en el conjunto de prueba. Se calculan métricas de rendimiento como el coeficiente de determinación (R2), el error absoluto medio (MAE) y el error cuadrático medio (RMSE). Estas métricas se registran en un DataFrame para comparar los diferentes algoritmos.\n",
        "<br>\n",
        "<br>\n",
        "**Resultados**\n",
        "\n",
        "Al final de este proceso, se obtiene un DataFrame que contiene el nombre del algoritmo, las métricas de rendimiento y los mejores hiperparámetros encontrados para cada uno. Esta comparativa permite identificar cuál de los algoritmos probados ofrece el mejor rendimiento en la tarea de predecir la semana de parto, tomando en cuenta tanto su precisión como su capacidad de generalización."
      ],
      "id": "2oKUz4UXD1x_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKiypt4q_k6X"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Integer, Categorical\n",
        "\n",
        "\n",
        "# Supongamos que 'x' y 'y' son tus datos de entrada y salida.\n",
        "\n",
        "# Definiendo el espacio de hiperparámetros para cada algoritmo.\n",
        "param_spaces = {\n",
        "   # 'LinearRegression': {},  # Não há hiperparâmetros essenciais a ajustar.\n",
        "  #  'SVR': {'C': Real(0.1, 1000, \"log-uniform\"), 'epsilon': Real(0.01, 1), 'kernel': Categorical(['linear', 'poly', 'rbf', 'sigmoid'])},\n",
        " #   'ElasticNet': {'alpha': Real(0.0001, 1, \"log-uniform\"), 'l1_ratio': Real(0, 1)},\n",
        " #   'DecisionTreeRegressor': {'max_depth': Integer(1, 30), 'min_samples_split': Integer(2, 20)},\n",
        " #   'BayesianRidge': {'alpha_1': Real(1e-6, 1e+6, \"log-uniform\"), 'lambda_1': Real(1e-6, 1e+6, \"log-uniform\")},\n",
        " #   'MLPRegressor': {'hidden_layer_sizes': Integer(10, 200), 'alpha': Real(0.0001, 1, \"log-uniform\"), 'learning_rate_init': Real(0.001, 0.1, \"log-uniform\")},\n",
        "    'GradientBoostingRegressor': {'n_estimators': Integer(100, 500), 'learning_rate': Real(0.01, 0.1), 'max_depth': Integer(3, 10)}\n",
        "}\n",
        "\n",
        "\n",
        "# Definiendo las columnas para el DataFrame que comparará los modelos.\n",
        "MLA_columns = ['MLA Name', 'R2', 'Mean Absolute Perc Error', 'Mean Absolute Error', 'Root Mean Squared Error', 'Best Hyperparameters']\n",
        "MLA_compare = pd.DataFrame(columns=MLA_columns)\n",
        "row_index = 0\n",
        "\n",
        "# Bucle para realizar la validación cruzada y la optimización bayesiana.\n",
        "for _ in range(5):\n",
        "    kf = KFold(2, shuffle=True)\n",
        "    for train_index, test_index in kf.split(x):\n",
        "        X_train, X_test = x.iloc[train_index], x.iloc[test_index]\n",
        "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "        for alg in tqdm(MLA):\n",
        "            alg_name = alg.__class__.__name__\n",
        "                # Verifica se o algoritmo atual tem hiperparámetros definidos.\n",
        "            if alg_name in param_spaces and param_spaces[alg_name]:\n",
        "                # Aplica la optimización bayesiana para encontrar los mejores hiperparámetros.\n",
        "                bayes_search = BayesSearchCV(alg, param_spaces[alg_name], n_iter=32, cv=2, n_jobs=-1, scoring='neg_mean_squared_error')\n",
        "                bayes_search.fit(X_train, y_train)\n",
        "\n",
        "                # Obteniendo el mejor modelo y sus hiperparámetros.\n",
        "                best_model = bayes_search.best_estimator_\n",
        "                best_params = bayes_search.best_params_\n",
        "            else:\n",
        "                # Entrena el modelo con los hiperparámetros por defecto.\n",
        "                best_model = alg.fit(X_train, y_train)\n",
        "                best_params = alg.get_params()\n",
        "\n",
        "            # Realiza la predicción y calcula las métricas de rendimiento.\n",
        "            predicted = best_model.predict(X_test)\n",
        "            MLA_compare.loc[row_index, 'MLA Name'] = alg_name\n",
        "            MLA_compare.loc[row_index, 'R2'] = round(r2_score(y_test, predicted), 2)\n",
        "            MLA_compare.loc[row_index, 'Mean Absolute Perc Error'] = round(np.mean(np.abs((y_test - predicted) / predicted)), 2)\n",
        "            MLA_compare.loc[row_index, 'Mean Absolute Error'] = round(mean_absolute_error(y_test, predicted), 2)\n",
        "            MLA_compare.loc[row_index, 'Root Mean Squared Error'] = round(np.sqrt(mean_squared_error(y_test, predicted)), 2)\n",
        "            MLA_compare.loc[row_index, 'Best Hyperparameters'] = str(best_params)\n",
        "            row_index += 1\n",
        "\n",
        "# Mostrando los resultados.\n",
        "MLA_compare\n",
        "\n",
        "#https://www.run.ai/guides/hyperparameter-tuning/bayesian-hyperparameter-optimization"
      ],
      "id": "iKiypt4q_k6X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8O_unKuD61O"
      },
      "source": [
        "### 4.4 Promedio de Métricas\n",
        "Tras probar múltiples iteraciones de cada algoritmo, se calculará el promedio de las métricas de rendimiento para cada uno. Esto permitirá identificar qué algoritmo, en promedio, ofrece un mejor desempeño para predecir la semana de parto. Esta evaluación media es crucial para obtener una visión más equilibrada y consistente del rendimiento de los modelos, ya que considera múltiples ejecuciones en lugar de un único resultado."
      ],
      "id": "j8O_unKuD61O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YiY-eEK0nTUj"
      },
      "outputs": [],
      "source": [
        "mean = MLA_compare.groupby(['MLA Name']).mean()\n",
        "mean = mean[['R2', 'Mean Absolute Perc Error','Mean Absolute Error', 'Root Mean Squared Error']]\n",
        "mean"
      ],
      "id": "YiY-eEK0nTUj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSn_yUeWEkrB"
      },
      "source": [
        "### 4.5 Prueba del Mejor Algoritmo\n",
        "Esta sección se enfoca en reproducir y evaluar el rendimiento del mejor modelo identificado en las pruebas anteriores. Se implementará el modelo con los hiperparámetros óptimos encontrados y se evaluará su capacidad predictiva."
      ],
      "id": "YSn_yUeWEkrB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xt5CiwNEnM5"
      },
      "source": [
        "#### 4.5.1 Separación de Entrenamiento y Pruebas\n",
        "Se dividirán los datos en dos conjuntos: uno para el entrenamiento (70%) y otro para la prueba (30%). Esta división es esencial para evaluar cómo el modelo se desempeña con datos no vistos durante su entrenamiento, lo que proporciona una medida de su capacidad de generalización."
      ],
      "id": "4Xt5CiwNEnM5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OntzajAMnkYp"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x,y,test_size=.30,random_state=42)\n",
        "print(\"Train size: \", len(X_train))\n",
        "print(\"Test size: \", len(X_test))"
      ],
      "id": "OntzajAMnkYp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvRzInRGErIT"
      },
      "source": [
        "#### 4.5.2 Construcción del Modelo\n",
        "En esta fase, se reconstruye el mejor modelo utilizando los hiperparámetros identificados previamente. Este paso es crucial para confirmar que el modelo seleccionado es efectivo y robusto bajo las condiciones óptimas establecidas."
      ],
      "id": "gvRzInRGErIT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqZ4oYEnn2KA"
      },
      "outputs": [],
      "source": [
        "personal_parameters = {\n",
        "    'loss': 'squared_error',\n",
        "    'learning_rate': 0.1,\n",
        "    'n_estimators': 100,\n",
        "    'subsample': 1.0,\n",
        "    'criterion': 'friedman_mse',\n",
        "    'min_samples_split': 2,\n",
        "    'min_samples_leaf': 1,\n",
        "    'min_weight_fraction_leaf': 0.0,\n",
        "    'max_depth': 3,\n",
        "    'min_impurity_decrease': 0.0,\n",
        "    'init': None,\n",
        "    'random_state': 42,\n",
        "    'max_features': None,\n",
        "    'alpha': 0.9,\n",
        "    'verbose': 0,\n",
        "    'max_leaf_nodes': None,\n",
        "    'warm_start': False,\n",
        "    'validation_fraction': 0.1,\n",
        "    'n_iter_no_change': None,\n",
        "    'tol': 0.0001,\n",
        "    'ccp_alpha': 0.0\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "MLA = [\n",
        "GradientBoostingRegressor(**personal_parameters)\n",
        "]\n",
        "\n",
        "MLA_columns = ['MLA Name', 'R2', 'Mean Absolute Perc Error', 'Mean Absolute Error', 'Root Mean Squared Error']\n",
        "MLA_best = pd.DataFrame(columns = MLA_columns)\n",
        "\n",
        "row_index = 0\n",
        "for alg in MLA:\n",
        "    model = alg.fit(X_train, y_train)\n",
        "    predicted = model.predict(X_test)\n",
        "    MLA_best.loc[row_index, 'MLA Name'] = alg.__class__.__name__\n",
        "    MLA_best.loc[row_index, 'R2'] = round(r2_score(y_test, predicted), 2)\n",
        "    MLA_best.loc[row_index, 'Mean Absolute Perc Error'] = round(np.mean(np.abs((y_test - predicted) / predicted)), 2)\n",
        "    MLA_best.loc[row_index, 'Mean Absolute Error'] = round(mean_absolute_error(y_test, predicted), 2)\n",
        "    MLA_best.loc[row_index, 'Root Mean Squared Error'] = round(np.sqrt(mean_squared_error(y_test, predicted)), 2)\n"
      ],
      "id": "yqZ4oYEnn2KA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjsNEevzExZ7"
      },
      "source": [
        "#### 4.5.3 Métricas del Modelo\n",
        "Se evaluarán las métricas del modelo utilizando el conjunto de prueba. Estas métricas proporcionarán una comprensión detallada del rendimiento del modelo en términos de precisión y capacidad de generalización."
      ],
      "id": "MjsNEevzExZ7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV0LWsYCoEPT"
      },
      "outputs": [],
      "source": [
        "MLA_best"
      ],
      "id": "AV0LWsYCoEPT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGcb0p_DE41P"
      },
      "source": [
        "#### 4.5.4 Visualización de la Capacidad Predictiva del Modelo\n",
        "Se realizará una visualización comparativa de los valores reales versus los valores predichos por el modelo. Esta comparación gráfica es clave para comprender visualmente cómo el modelo predice la semana de parto y dónde puede tener dificultades."
      ],
      "id": "qGcb0p_DE41P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KirYzcIVaJhw"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(y_test, predicted, alpha=0.5, color='blue', label='Dados Preditos')\n",
        "plt.plot(y_test, y_test, color='red', label='Linha Ideal (Dados Reais)')\n",
        "plt.title('Comparación de las semanas reales con las estimadas')\n",
        "plt.xlabel('Semanas Estimadas')\n",
        "plt.ylabel('Semana Reales')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "KirYzcIVaJhw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H97EIdUaE9KJ"
      },
      "source": [
        "#### 4.5.5 Visualización del Error Predictivo\n",
        "Se mostrará la distribución del error de predicción. Idealmente, esta distribución debería estar concentrada cerca de cero, con la menor dispersión posible en el eje Y, indicando errores pequeños y consistentes."
      ],
      "id": "H97EIdUaE9KJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCO-x44Ko3Kd"
      },
      "outputs": [],
      "source": [
        "sem_error = y_test - predicted\n",
        "sem_error.hist(bins=30)\n",
        "plt.title('Prediction Error Histogram')"
      ],
      "id": "JCO-x44Ko3Kd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dys6L3gXFA6A"
      },
      "source": [
        "#### 4.5.6 Importancia de las Características\n",
        "Esta sección abordará la influencia de cada variable predictora en el modelo. Se utilizará el concepto de \"importancia de las características\" para identificar qué variables tienen un mayor impacto en las predicciones. Un valor más alto en la importancia de la característica indica una mayor influencia en el rendimiento predictivo del modelo. Esta información es crucial para entender qué factores son más relevantes a la hora de predecir la semana de parto."
      ],
      "id": "Dys6L3gXFA6A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaN_I4sbo9Z3"
      },
      "outputs": [],
      "source": [
        "feature_importance = model.feature_importances_\n",
        "feature_importance = feature_importance[feature_importance > 0.005]\n",
        "sorted_idx = np.argsort(feature_importance)\n",
        "pos = np.arange(sorted_idx.shape[0])\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
        "plt.yticks(pos, np.array(x.columns)[sorted_idx])\n",
        "plt.title('Feature Importance')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "id": "SaN_I4sbo9Z3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWiXWK6OgSwP"
      },
      "source": [
        "#### 4.5.7 Comprendiendo el Modelo a un Nivel Más Profundo\n",
        "Finalmente, se utilizará el análisis SHAP (Shapley Additive exPlanations) para comprender en detalle cómo cada característica influye en las predicciones del modelo. A través de gráficos de cascada (waterfall), se podrá visualizar no solo la importancia, sino también la dirección de la influencia (aumentando o disminuyendo la predicción) de cada variable.\n",
        "\n",
        "**¿Qué es SHAP y Cómo Funciona?**\n",
        "\n",
        "SHAP es un enfoque de teoría de juegos para explicar el resultado de cualquier modelo de machine learning. Proporciona valores que explican el impacto de tener una cierta característica en la predicción comparada con la predicción promedio del modelo. Estos valores pueden ser positivos o negativos, dependiendo de si la característica aumenta o disminuye la predicción.\n",
        "\n",
        "**Interpretación de Gráficos de Cascada (Waterfall) en SHAP**\n",
        "\n",
        "Los gráficos de cascada muestran el impacto de cada característica en la predicción final. Comienzan con la predicción base (la media de todas las predicciones) y luego añaden el efecto de cada característica, mostrando cómo cada una contribuye a alejarse de la predicción base hacia la predicción final. Esto permite una comprensión visual intuitiva de cómo y por qué el modelo está haciendo sus predicciones, facilitando la interpretación y la toma de decisiones basadas en el modelo.\n",
        "\n",
        "Más información:  https://www.aidancooper.co.uk/a-non-technical-guide-to-interpreting-shap-analyses/"
      ],
      "id": "mWiXWK6OgSwP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPTLbszfpi6f"
      },
      "outputs": [],
      "source": [
        "# Calcular os valores SHAP\n",
        "explainer = shap.Explainer(model, X_train)\n",
        "shap_values = explainer(X_test)\n",
        "\n",
        "#shap.summary_plot(shap_values, X_test)\n",
        "shap.plots.waterfall(shap_values[0])\n"
      ],
      "id": "XPTLbszfpi6f"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "QBZ0Z0BaWo_v",
        "HjYEJo1QWm36"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}